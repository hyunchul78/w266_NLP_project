{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ines/anaconda3/envs/tensorflow_gpu_1_13/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ines/anaconda3/envs/tensorflow_gpu_1_13/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ines/anaconda3/envs/tensorflow_gpu_1_13/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ines/anaconda3/envs/tensorflow_gpu_1_13/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ines/anaconda3/envs/tensorflow_gpu_1_13/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ines/anaconda3/envs/tensorflow_gpu_1_13/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Bidirectional, Dense, Embedding, Input, Lambda, LSTM, RepeatVector, TimeDistributed, Layer, Activation, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import ELU\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from scipy import spatial\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 18258029436572050982\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 2049744545223707271\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 12260937512979157870\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 9728249037\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 236946898889999262\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directories and text loading\n",
    "Initially we will set the main directories and some variables regarding the characteristics of our texts.\n",
    "We set the maximum sequence length to 15, the maximun number of words in our vocabulary to 12000 and we will use 50-dimensional embeddings. Finally we load our texts from a csv. The text file is the train file of the Quora Kaggle challenge containing around 808000 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV (Comma Separated Values)\n",
    "Format for spreadsheets and databases\n",
    "- reader object: Return a reader object which will iterate over lines in the given csvfile.\n",
    "\n",
    "- writer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 808580 texts in train.csv\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = '/home/ines/nlp_project/text_VAE/'\n",
    "TRAIN_DATA_FILE = BASE_DIR + 'train.csv'#'train_micro.csv'\n",
    "GLOVE_EMBEDDING = BASE_DIR + 'GloVe/glove.6B.50d.txt'\n",
    "VALIDATION_SPLIT = 0.2\n",
    "MAX_SEQUENCE_LENGTH = 15\n",
    "MAX_NB_WORDS = 12000\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "texts = [] \n",
    "with codecs.open(TRAIN_DATA_FILE, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    for values in reader:\n",
    "        texts.append(values[3])\n",
    "        texts.append(values[4])\n",
    "print('Found %s texts in train.csv' % len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "To preprocess the text we will use the tokenizer and the text_to_sequences function from Keras\n",
    "\n",
    "\n",
    "- `Tokenizer(num_words)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 95596 unique tokens\n",
      "Shape of data tensor: (808580, 15)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# {key=word : value:index}\n",
    "word_index = tokenizer.word_index #the dict values start from 1 so this is fine with zeropadding\n",
    "# {key=index : value:word}\n",
    "index2word = {v: k for k, v in word_index.items()}\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "# Return index list of Char array \n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "# Return (sample, MAX_SEQUENCE_LENGTH) size integer tensor\n",
    "data_1 = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', data_1.shape)\n",
    "NB_WORDS = (min(tokenizer.num_words, len(word_index)) + 1 ) #+1 for zero padding\n",
    "data_1_val = data_1[801000:807000] #select 6000 sentences as validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the step by step guide to invest in share market in india?\n",
      "[2, 3, 1, 1222, 57, 1222, 2581, 7, 576, 8, 763, 383, 8, 35]\n",
      "What does it mean when a guy says I like you?\n",
      "[2, 21, 19, 101, 37, 6, 287, 716, 5, 39, 15]\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])\n",
    "print(sequences[0])\n",
    "\n",
    "print(texts[1000])\n",
    "print(sequences[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence generator\n",
    "In order to reduce the memory requirements we will gradually read our sentences from the csv through Pandas as we feed them to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_generator(TRAIN_DATA_FILE, chunksize):\n",
    "    reader = pd.read_csv(TRAIN_DATA_FILE, chunksize=chunksize, iterator=True)\n",
    "    for df in reader:\n",
    "        #print(df.shape)\n",
    "        #df=pd.read_csv(TRAIN_DATA_FILE, iterator=False)\n",
    "        val3 = df.iloc[:,3:4].values.tolist()\n",
    "        val4 = df.iloc[:,4:5].values.tolist()\n",
    "        flat3 = [item for sublist in val3 for item in sublist]\n",
    "        flat4 = [str(item) for sublist in val4 for item in sublist]\n",
    "        texts = [] \n",
    "        # 'Append' add x, but extend add x's elements\n",
    "        texts.extend(flat3[:]) # Column Question1\n",
    "        texts.extend(flat4[:]) # Column Question2\n",
    "        sequences = tokenizer.texts_to_sequences(texts)\n",
    "        data_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "        yield (data_train, data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings\n",
    "We will use pretrained Glove word embeddings as embeddings for our network. We create a matrix with one embedding for every word in our vocabulary and then we will pass this matrix as weights to the keras embedding layer of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Null word embeddings: 1\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(GLOVE_EMBEDDING, encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "glove_embedding_matrix = np.zeros((NB_WORDS, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i < NB_WORDS:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be the word embedding of 'unk'.\n",
    "            glove_embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            glove_embedding_matrix[i] = embeddings_index.get('unk')\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(glove_embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE model\n",
    "Our model is based on a seq2seq architecture with a bidirectional LSTM encoder and an LSTM decoder and ELU activations.\n",
    "We feed the latent representation at every timestep as input to the decoder through \"RepeatVector(max_len)\".\n",
    "To avoid the one-hot representation of labels we use the \"tf.contrib.seq2seq.sequence_loss\" that requires as labels only the word indexes (the same that go in input to the embedding matrix) and calculates internally the final softmax (so the model ends with a dense layer with linear activation). Optionally the \"sequence_loss\" allows to use the sampled softmax which helps when dealing with large vocabularies (for example with a 50k words vocabulary) but in this I didn't use it.\n",
    "Moreover, due to the pandas iterator that reads the csv both the train size and validation size must be divisible by the batch_size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponential Linear Unit or its widely known name ELU is a function that tend to converge cost to zero faster and produce more accurate results. Different to other activation functions, ELU has a extra alpha constant which should be positive number.\n",
    "\n",
    "ELU is very similiar to RELU except negative inputs. They are both in identity function form for non-negative inputs. On the other hand, ELU becomes smooth slowly until its output equal to -α whereas RELU sharply smoothes.\n",
    "\n",
    "https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#elu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ines/anaconda3/envs/tensorflow_gpu_1_13/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ines/anaconda3/envs/tensorflow_gpu_1_13/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:4010: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "(?, 15) (100, 15, 12001)\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 15, 50)       600050      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 192)          112896      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 192)          0           bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 96)           18528       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "elu (ELU)                       (None, 96)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 96)           0           elu[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           3104        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           3104        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (100, 32)            0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (100, 15, 32)        0           lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (100, 15, 96)        49536       repeat_vector_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (100, 15, 12001)     1164097     lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "custom_variational_layer (Custo (None, 15)           0           input_1[0][0]                    \n",
      "                                                                 time_distributed[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 1,951,315\n",
      "Trainable params: 1,351,265\n",
      "Non-trainable params: 600,050\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "max_len = MAX_SEQUENCE_LENGTH\n",
    "emb_dim = EMBEDDING_DIM\n",
    "latent_dim = 32\n",
    "intermediate_dim = 96\n",
    "epsilon_std = 1.0\n",
    "num_sampled=500\n",
    "act = ELU()\n",
    "repeated_context = RepeatVector(max_len)\n",
    "\n",
    "#y = Input(batch_shape=(None, max_len, NB_WORDS))\n",
    "x = Input(batch_shape=(None, max_len))\n",
    "x_embed = Embedding(NB_WORDS, emb_dim, weights=[glove_embedding_matrix],\n",
    "                            input_length=max_len, trainable=False)(x)\n",
    "h = Bidirectional(LSTM(intermediate_dim, return_sequences=False, recurrent_dropout=0.2), merge_mode='concat')(x_embed)\n",
    "h = Dropout(0.2)(h)\n",
    "h = Dense(intermediate_dim, activation='linear')(h)\n",
    "h = act(h)\n",
    "h = Dropout(0.2)(h)\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_var = Dense(latent_dim)(h)\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "# we instantiate these layers separately so as to reuse them later\n",
    "repeated_context = RepeatVector(max_len)\n",
    "decoder_h = LSTM(intermediate_dim, return_sequences=True, recurrent_dropout=0.2)\n",
    "decoder_mean = TimeDistributed(Dense(NB_WORDS, activation='linear'))#softmax is applied in the seq2seqloss by tf\n",
    "\n",
    "h_decoded = decoder_h(repeated_context(z))\n",
    "x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "\n",
    "# placeholder loss\n",
    "def zero_loss(y_true, y_pred):\n",
    "    return K.zeros_like(y_pred)\n",
    "\n",
    "#=========================== Necessary only if you want to use Sampled Softmax =======================#\n",
    "#Sampled softmax\n",
    "logits = tf.constant(np.random.randn(batch_size, max_len, NB_WORDS), tf.float32)\n",
    "targets = tf.constant(np.random.randint(NB_WORDS, size=(batch_size, max_len)), tf.int32)\n",
    "proj_w = tf.constant(np.random.randn(NB_WORDS, NB_WORDS), tf.float32)\n",
    "proj_b = tf.constant(np.zeros(NB_WORDS), tf.float32)\n",
    "\n",
    "def _sampled_loss(labels, logits):\n",
    "    labels = tf.cast(labels, tf.int64)\n",
    "    labels = tf.reshape(labels, [-1, 1])\n",
    "    logits = tf.cast(logits, tf.float32)\n",
    "    return tf.cast(\n",
    "                    tf.nn.sampled_softmax_loss(\n",
    "                        proj_w,\n",
    "                        proj_b,\n",
    "                        labels,\n",
    "                        logits,\n",
    "                        num_sampled=num_sampled,\n",
    "                        num_classes=NB_WORDS),\n",
    "                    tf.float32)\n",
    "softmax_loss_f = _sampled_loss\n",
    "#====================================================================================================#\n",
    "\n",
    "# Custom VAE loss layer\n",
    "class CustomVariationalLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "        self.target_weights = tf.constant(np.ones((batch_size, max_len)), tf.float32)\n",
    "\n",
    "    def vae_loss(self, x, x_decoded_mean):\n",
    "        #xent_loss = K.sum(metrics.categorical_crossentropy(x, x_decoded_mean), axis=-1)\n",
    "        labels = tf.cast(x, tf.int32)\n",
    "        xent_loss = K.sum(tf.contrib.seq2seq.sequence_loss(x_decoded_mean, labels, \n",
    "                                                     weights=self.target_weights,\n",
    "                                                     average_across_timesteps=False,\n",
    "                                                     average_across_batch=False), axis=-1)\n",
    "                                                     #softmax_loss_function=softmax_loss_f), axis=-1)#, uncomment for sampled doftmax\n",
    "        kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        return K.mean(xent_loss + kl_loss)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        x_decoded_mean = inputs[1]\n",
    "        print(x.shape, x_decoded_mean.shape)\n",
    "        loss = self.vae_loss(x, x_decoded_mean)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # we don't use this output, but it has to have the correct shape:\n",
    "        return K.ones_like(x)\n",
    "\n",
    "loss_layer = CustomVariationalLayer()([x, x_decoded_mean])\n",
    "vae = Model(x, [loss_layer])\n",
    "opt = Adam(lr=0.01) #SGD(lr=1e-2, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "vae.compile(optimizer='adam', loss=[zero_loss])\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "We train our model for 100 epochs through keras \".fit_generator\". The number of steps per epoch is equal to the number of sentences that we have in the train set (800000) divided by the batch size; the additional /2 is due to the fact that our csv has two sentnces per line so in the end we have to read with our generator only 400000 lines per epoch.\n",
    "For validation data we pass the same array twice since input and labels of this model are the same. \n",
    "If we didn't use the \"tf.contrib.seq2seq.sequence_loss\" (or another similar function) we would have had to pass as labels the sequence of word one-hot encodings with dimension (batch_size, seq_len, vocab_size) consuming a lot of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from tensorflow.python.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "callback = TensorBoard(log_dir)\n",
    "callback.set_model([vae])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_log(callback, names, logs, batch_no):\n",
    "\n",
    "    summary = tf.Summary()\n",
    "    summary_value = summary.value.add()\n",
    "    summary_value.simple_value = logs[0].history['loss'][0]\n",
    "    summary_value.tag = names[0]\n",
    "    callback.writer.add_summary(summary, batch_no)\n",
    "\n",
    "    summary = tf.Summary()\n",
    "    summary_value = summary.value.add()\n",
    "    summary_value.simple_value = logs[0].history['val_loss'][0]\n",
    "    summary_value.tag = names[1]\n",
    "    callback.writer.add_summary(summary, batch_no)\n",
    "    callback.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------epoch:  0 --------\n",
      "WARNING:tensorflow:From /home/ines/anaconda3/envs/tensorflow_gpu_1_13/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "6000/6000 [==============================] - 1s 204us/sample - loss: 65.2099\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 156s 39ms/step - loss: 72.2730 - val_loss: 65.2099\n",
      "-------epoch:  1 --------\n",
      "6000/6000 [==============================] - 1s 174us/sample - loss: 63.7823\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 159s 40ms/step - loss: 64.7935 - val_loss: 63.7823\n",
      "-------epoch:  2 --------\n",
      "6000/6000 [==============================] - 1s 174us/sample - loss: 62.7355\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 161s 40ms/step - loss: 63.6107 - val_loss: 62.7355\n",
      "-------epoch:  3 --------\n",
      "6000/6000 [==============================] - 1s 169us/sample - loss: 62.2140\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 159s 40ms/step - loss: 62.8898 - val_loss: 62.2140\n",
      "-------epoch:  4 --------\n",
      "6000/6000 [==============================] - 2s 276us/sample - loss: 61.9130\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 160s 40ms/step - loss: 62.4800 - val_loss: 61.9130\n",
      "-------epoch:  5 --------\n",
      "6000/6000 [==============================] - 1s 160us/sample - loss: 61.7625\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 158s 39ms/step - loss: 62.2325 - val_loss: 61.7625\n",
      "-------epoch:  6 --------\n",
      "6000/6000 [==============================] - 1s 162us/sample - loss: 61.6358\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 153s 38ms/step - loss: 62.0499 - val_loss: 61.6358\n",
      "-------epoch:  7 --------\n",
      "6000/6000 [==============================] - 1s 161us/sample - loss: 61.5660\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 152s 38ms/step - loss: 61.9188 - val_loss: 61.5660\n",
      "-------epoch:  8 --------\n",
      "6000/6000 [==============================] - 1s 163us/sample - loss: 61.4176\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 152s 38ms/step - loss: 61.7812 - val_loss: 61.4176\n",
      "-------epoch:  9 --------\n",
      "6000/6000 [==============================] - 1s 168us/sample - loss: 61.3385\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 152s 38ms/step - loss: 61.6567 - val_loss: 61.3385\n",
      "-------epoch:  10 --------\n",
      "6000/6000 [==============================] - 1s 161us/sample - loss: 61.2639\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 156s 39ms/step - loss: 61.5483 - val_loss: 61.2639\n",
      "-------epoch:  11 --------\n",
      "6000/6000 [==============================] - 1s 164us/sample - loss: 61.1932\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 161s 40ms/step - loss: 61.4618 - val_loss: 61.1932\n",
      "-------epoch:  12 --------\n",
      "6000/6000 [==============================] - 1s 182us/sample - loss: 61.2692\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 163s 41ms/step - loss: 61.3817 - val_loss: 61.2692\n",
      "-------epoch:  13 --------\n",
      "6000/6000 [==============================] - 1s 161us/sample - loss: 61.1136\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 167s 42ms/step - loss: 61.2850 - val_loss: 61.1136\n",
      "-------epoch:  14 --------\n",
      "6000/6000 [==============================] - 1s 203us/sample - loss: 61.0220\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 157s 39ms/step - loss: 61.1979 - val_loss: 61.0220\n",
      "-------epoch:  15 --------\n",
      "6000/6000 [==============================] - 1s 163us/sample - loss: 61.1253\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 156s 39ms/step - loss: 61.1272 - val_loss: 61.1253\n",
      "-------epoch:  16 --------\n",
      "6000/6000 [==============================] - 1s 165us/sample - loss: 60.8690\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 155s 39ms/step - loss: 61.0415 - val_loss: 60.8690\n",
      "-------epoch:  17 --------\n",
      "6000/6000 [==============================] - 1s 162us/sample - loss: 60.8758\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 154s 39ms/step - loss: 60.9724 - val_loss: 60.8758\n",
      "-------epoch:  18 --------\n",
      "6000/6000 [==============================] - 1s 163us/sample - loss: 60.7724\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 159s 40ms/step - loss: 60.9039 - val_loss: 60.7724\n",
      "-------epoch:  19 --------\n",
      "6000/6000 [==============================] - 1s 161us/sample - loss: 60.7362\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 162s 40ms/step - loss: 60.8363 - val_loss: 60.7362\n",
      "-------epoch:  20 --------\n",
      "6000/6000 [==============================] - 1s 168us/sample - loss: 60.7159\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 157s 39ms/step - loss: 60.7852 - val_loss: 60.7159\n",
      "-------epoch:  21 --------\n",
      "6000/6000 [==============================] - 1s 181us/sample - loss: 60.6453\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 159s 40ms/step - loss: 60.7303 - val_loss: 60.6453\n",
      "-------epoch:  22 --------\n",
      "6000/6000 [==============================] - 1s 166us/sample - loss: 60.5865\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 156s 39ms/step - loss: 60.6715 - val_loss: 60.5865\n",
      "-------epoch:  23 --------\n",
      "6000/6000 [==============================] - 1s 172us/sample - loss: 60.4834\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 159s 40ms/step - loss: 60.6242 - val_loss: 60.4834\n",
      "-------epoch:  24 --------\n",
      "6000/6000 [==============================] - 1s 165us/sample - loss: 60.4194\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 158s 40ms/step - loss: 60.5530 - val_loss: 60.4194\n",
      "-------epoch:  25 --------\n",
      "6000/6000 [==============================] - 1s 169us/sample - loss: 60.3713\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 159s 40ms/step - loss: 60.5032 - val_loss: 60.3713\n",
      "-------epoch:  26 --------\n",
      "6000/6000 [==============================] - 1s 169us/sample - loss: 60.3425\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 153s 38ms/step - loss: 60.4362 - val_loss: 60.3425\n",
      "-------epoch:  27 --------\n",
      "6000/6000 [==============================] - 1s 175us/sample - loss: 60.1836\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 172s 43ms/step - loss: 60.3809 - val_loss: 60.1836\n",
      "-------epoch:  28 --------\n",
      "6000/6000 [==============================] - 1s 163us/sample - loss: 60.1833\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 156s 39ms/step - loss: 60.3286 - val_loss: 60.1833\n",
      "-------epoch:  29 --------\n",
      "6000/6000 [==============================] - 1s 171us/sample - loss: 60.1231\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 156s 39ms/step - loss: 60.2714 - val_loss: 60.1231\n",
      "-------epoch:  30 --------\n",
      "6000/6000 [==============================] - 1s 163us/sample - loss: 60.2042\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 158s 40ms/step - loss: 60.2302 - val_loss: 60.2042\n",
      "-------epoch:  31 --------\n",
      "6000/6000 [==============================] - 1s 168us/sample - loss: 60.1650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 156s 39ms/step - loss: 60.1910 - val_loss: 60.1650\n",
      "-------epoch:  32 --------\n",
      "6000/6000 [==============================] - 1s 167us/sample - loss: 60.0403\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 156s 39ms/step - loss: 60.1506 - val_loss: 60.0403\n",
      "-------epoch:  33 --------\n",
      "6000/6000 [==============================] - 1s 161us/sample - loss: 60.1535\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 156s 39ms/step - loss: 60.1060 - val_loss: 60.1535\n",
      "-------epoch:  34 --------\n",
      "6000/6000 [==============================] - 1s 163us/sample - loss: 59.9789\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 166s 41ms/step - loss: 60.0765 - val_loss: 59.9789\n",
      "-------epoch:  35 --------\n",
      "6000/6000 [==============================] - 1s 164us/sample - loss: 60.1604\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 153s 38ms/step - loss: 60.0315 - val_loss: 60.1604\n",
      "-------epoch:  36 --------\n",
      "6000/6000 [==============================] - 1s 163us/sample - loss: 59.8783\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 153s 38ms/step - loss: 59.9985 - val_loss: 59.8783\n",
      "-------epoch:  37 --------\n",
      "6000/6000 [==============================] - 1s 164us/sample - loss: 59.9255\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 152s 38ms/step - loss: 59.9702 - val_loss: 59.9255\n",
      "-------epoch:  38 --------\n",
      "6000/6000 [==============================] - 1s 161us/sample - loss: 59.8502\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 152s 38ms/step - loss: 59.9345 - val_loss: 59.8502\n",
      "-------epoch:  39 --------\n",
      "6000/6000 [==============================] - 1s 164us/sample - loss: 59.8485\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 153s 38ms/step - loss: 59.9000 - val_loss: 59.8485\n",
      "-------epoch:  40 --------\n",
      "6000/6000 [==============================] - 1s 162us/sample - loss: 59.8874\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 153s 38ms/step - loss: 59.8778 - val_loss: 59.8874\n",
      "-------epoch:  41 --------\n",
      "6000/6000 [==============================] - 1s 164us/sample - loss: 59.8277\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 153s 38ms/step - loss: 59.8487 - val_loss: 59.8277\n",
      "-------epoch:  42 --------\n",
      "6000/6000 [==============================] - 1s 163us/sample - loss: 59.8755\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 153s 38ms/step - loss: 59.8241 - val_loss: 59.8755\n",
      "-------epoch:  43 --------\n",
      "6000/6000 [==============================] - 1s 171us/sample - loss: 59.8639\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 156s 39ms/step - loss: 59.8046 - val_loss: 59.8639\n",
      "-------epoch:  44 --------\n",
      "6000/6000 [==============================] - 1s 172us/sample - loss: 59.8272\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 158s 40ms/step - loss: 59.7862 - val_loss: 59.8272\n",
      "-------epoch:  45 --------\n",
      "6000/6000 [==============================] - 1s 170us/sample - loss: 59.8826\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 158s 39ms/step - loss: 59.7602 - val_loss: 59.8826\n",
      "-------epoch:  46 --------\n",
      "6000/6000 [==============================] - 1s 170us/sample - loss: 59.8354\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 158s 39ms/step - loss: 59.7496 - val_loss: 59.8354\n",
      "-------epoch:  47 --------\n",
      "6000/6000 [==============================] - 1s 172us/sample - loss: 59.8685\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 158s 39ms/step - loss: 59.7252 - val_loss: 59.8685\n",
      "-------epoch:  48 --------\n",
      "6000/6000 [==============================] - 1s 191us/sample - loss: 59.8607\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 158s 40ms/step - loss: 59.7121 - val_loss: 59.8607\n",
      "-------epoch:  49 --------\n",
      "6000/6000 [==============================] - 1s 162us/sample - loss: 59.8833\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 158s 40ms/step - loss: 59.6912 - val_loss: 59.8833\n",
      "-------epoch:  50 --------\n",
      "6000/6000 [==============================] - 1s 165us/sample - loss: 59.9232\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 153s 38ms/step - loss: 59.6756 - val_loss: 59.9232\n",
      "-------epoch:  51 --------\n",
      "6000/6000 [==============================] - 1s 163us/sample - loss: 59.8262\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 153s 38ms/step - loss: 59.6598 - val_loss: 59.8262\n",
      "-------epoch:  52 --------\n",
      "6000/6000 [==============================] - 1s 164us/sample - loss: 60.0193\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 153s 38ms/step - loss: 59.6404 - val_loss: 60.0193\n",
      "-------epoch:  53 --------\n",
      "6000/6000 [==============================] - 1s 162us/sample - loss: 59.9292\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 153s 38ms/step - loss: 59.6353 - val_loss: 59.9292\n",
      "-------epoch:  54 --------\n",
      "6000/6000 [==============================] - 1s 164us/sample - loss: 59.9196\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 153s 38ms/step - loss: 59.6186 - val_loss: 59.9196\n",
      "-------epoch:  55 --------\n",
      "6000/6000 [==============================] - 1s 164us/sample - loss: 59.8582\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 153s 38ms/step - loss: 59.5977 - val_loss: 59.8582\n",
      "-------epoch:  56 --------\n",
      "6000/6000 [==============================] - 1s 164us/sample - loss: 59.8195\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 153s 38ms/step - loss: 59.5928 - val_loss: 59.8195\n",
      "-------epoch:  57 --------\n",
      "6000/6000 [==============================] - 1s 161us/sample - loss: 59.8439\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 152s 38ms/step - loss: 59.5857 - val_loss: 59.8439\n",
      "-------epoch:  58 --------\n",
      "6000/6000 [==============================] - 1s 161us/sample - loss: 59.8166\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 152s 38ms/step - loss: 59.5698 - val_loss: 59.8166\n",
      "-------epoch:  59 --------\n",
      "6000/6000 [==============================] - 1s 161us/sample - loss: 59.8958\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 151s 38ms/step - loss: 59.5605 - val_loss: 59.8958\n",
      "-------epoch:  60 --------\n",
      "6000/6000 [==============================] - 1s 161us/sample - loss: 59.9759\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 152s 38ms/step - loss: 59.5434 - val_loss: 59.9759\n",
      "-------epoch:  61 --------\n",
      "6000/6000 [==============================] - 1s 165us/sample - loss: 59.8695\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 152s 38ms/step - loss: 59.5347 - val_loss: 59.8695\n",
      "-------epoch:  62 --------\n",
      "6000/6000 [==============================] - 1s 161us/sample - loss: 59.8353\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 152s 38ms/step - loss: 59.5201 - val_loss: 59.8353\n",
      "-------epoch:  63 --------\n",
      "6000/6000 [==============================] - 1s 161us/sample - loss: 59.8631\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 152s 38ms/step - loss: 59.5159 - val_loss: 59.8631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------epoch:  64 --------\n",
      "6000/6000 [==============================] - 1s 161us/sample - loss: 60.1333\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 152s 38ms/step - loss: 59.5048 - val_loss: 60.1333\n",
      "-------epoch:  65 --------\n",
      "6000/6000 [==============================] - 1s 161us/sample - loss: 59.8377\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 152s 38ms/step - loss: 59.4991 - val_loss: 59.8377\n",
      "-------epoch:  66 --------\n",
      "6000/6000 [==============================] - 1s 162us/sample - loss: 59.8367\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 152s 38ms/step - loss: 59.4830 - val_loss: 59.8367\n",
      "-------epoch:  67 --------\n",
      "6000/6000 [==============================] - 1s 161us/sample - loss: 59.8986\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 152s 38ms/step - loss: 59.4731 - val_loss: 59.8986\n",
      "-------epoch:  68 --------\n",
      "6000/6000 [==============================] - 1s 162us/sample - loss: 59.8220\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 152s 38ms/step - loss: 59.4680 - val_loss: 59.8220\n",
      "-------epoch:  69 --------\n",
      "6000/6000 [==============================] - 1s 160us/sample - loss: 59.7927\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 152s 38ms/step - loss: 59.4630 - val_loss: 59.7927\n",
      "-------epoch:  70 --------\n",
      "6000/6000 [==============================] - 1s 161us/sample - loss: 59.8569\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 151s 38ms/step - loss: 59.4572 - val_loss: 59.8569\n",
      "-------epoch:  71 --------\n",
      "6000/6000 [==============================] - 1s 162us/sample - loss: 60.1796\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 151s 38ms/step - loss: 59.4519 - val_loss: 60.1796\n",
      "-------epoch:  72 --------\n",
      "6000/6000 [==============================] - 1s 160us/sample - loss: 59.9202\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 152s 38ms/step - loss: 59.4344 - val_loss: 59.9202\n",
      "-------epoch:  73 --------\n",
      "6000/6000 [==============================] - 1s 162us/sample - loss: 59.8494\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 151s 38ms/step - loss: 59.4311 - val_loss: 59.8494\n",
      "-------epoch:  74 --------\n",
      "6000/6000 [==============================] - 1s 162us/sample - loss: 59.8235\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 151s 38ms/step - loss: 59.4234 - val_loss: 59.8235\n",
      "-------epoch:  75 --------\n",
      "6000/6000 [==============================] - 1s 162us/sample - loss: 59.7967\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 151s 38ms/step - loss: 59.4232 - val_loss: 59.7967\n",
      "-------epoch:  76 --------\n",
      "6000/6000 [==============================] - 1s 161us/sample - loss: 59.8016\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 151s 38ms/step - loss: 59.4125 - val_loss: 59.8016\n",
      "-------epoch:  77 --------\n",
      "6000/6000 [==============================] - 1s 161us/sample - loss: 59.8551\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 151s 38ms/step - loss: 59.4110 - val_loss: 59.8551\n",
      "-------epoch:  78 --------\n",
      "6000/6000 [==============================] - 1s 161us/sample - loss: 59.7965\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 151s 38ms/step - loss: 59.3907 - val_loss: 59.7965\n",
      "-------epoch:  79 --------\n",
      "6000/6000 [==============================] - 1s 163us/sample - loss: 59.7897\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 152s 38ms/step - loss: 59.3828 - val_loss: 59.7897\n",
      "-------epoch:  80 --------\n",
      "6000/6000 [==============================] - 1s 166us/sample - loss: 59.8276\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 151s 38ms/step - loss: 59.3901 - val_loss: 59.8276\n",
      "-------epoch:  81 --------\n",
      "6000/6000 [==============================] - 1s 161us/sample - loss: 59.7725\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 152s 38ms/step - loss: 59.3956 - val_loss: 59.7725\n",
      "-------epoch:  82 --------\n",
      "6000/6000 [==============================] - 1s 161us/sample - loss: 59.7847\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 151s 38ms/step - loss: 59.3749 - val_loss: 59.7847\n",
      "-------epoch:  83 --------\n",
      "6000/6000 [==============================] - 1s 161us/sample - loss: 59.8309\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 151s 38ms/step - loss: 59.3695 - val_loss: 59.8309\n",
      "-------epoch:  84 --------\n",
      "6000/6000 [==============================] - 1s 161us/sample - loss: 59.7969\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 152s 38ms/step - loss: 59.3696 - val_loss: 59.7969\n",
      "-------epoch:  85 --------\n",
      "6000/6000 [==============================] - 1s 162us/sample - loss: 59.8404\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 152s 38ms/step - loss: 59.3558 - val_loss: 59.8404\n",
      "-------epoch:  86 --------\n",
      "6000/6000 [==============================] - 1s 161us/sample - loss: 59.7877\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 151s 38ms/step - loss: 59.3556 - val_loss: 59.7877\n",
      "-------epoch:  87 --------\n",
      "6000/6000 [==============================] - 1s 161us/sample - loss: 60.2101\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 151s 38ms/step - loss: 59.3481 - val_loss: 60.2101\n",
      "-------epoch:  88 --------\n",
      "6000/6000 [==============================] - 1s 161us/sample - loss: 60.0949\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 151s 38ms/step - loss: 59.3376 - val_loss: 60.0949\n",
      "-------epoch:  89 --------\n",
      "6000/6000 [==============================] - 1s 161us/sample - loss: 59.7709\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 152s 38ms/step - loss: 59.3353 - val_loss: 59.7709\n",
      "-------epoch:  90 --------\n",
      "6000/6000 [==============================] - 1s 161us/sample - loss: 59.7778\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 151s 38ms/step - loss: 59.3259 - val_loss: 59.7778\n",
      "-------epoch:  91 --------\n",
      "6000/6000 [==============================] - 1s 161us/sample - loss: 59.8290\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 152s 38ms/step - loss: 59.3211 - val_loss: 59.8290\n",
      "-------epoch:  92 --------\n",
      "6000/6000 [==============================] - 1s 161us/sample - loss: 59.8268\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 152s 38ms/step - loss: 59.3225 - val_loss: 59.8268\n",
      "-------epoch:  93 --------\n",
      "6000/6000 [==============================] - 1s 161us/sample - loss: 60.3653\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 152s 38ms/step - loss: 59.3175 - val_loss: 60.3653\n",
      "-------epoch:  94 --------\n",
      "6000/6000 [==============================] - 1s 160us/sample - loss: 59.8169\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 152s 38ms/step - loss: 59.3097 - val_loss: 59.8169\n",
      "-------epoch:  95 --------\n",
      "6000/6000 [==============================] - 1s 161us/sample - loss: 59.7918\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 151s 38ms/step - loss: 59.3117 - val_loss: 59.7918\n",
      "-------epoch:  96 --------\n",
      "6000/6000 [==============================] - 1s 162us/sample - loss: 59.8267\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 151s 38ms/step - loss: 59.3010 - val_loss: 59.8267\n",
      "-------epoch:  97 --------\n",
      "6000/6000 [==============================] - 1s 161us/sample - loss: 59.8079\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 151s 38ms/step - loss: 59.2916 - val_loss: 59.8079\n",
      "-------epoch:  98 --------\n",
      "6000/6000 [==============================] - 1s 162us/sample - loss: 59.8819\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 152s 38ms/step - loss: 59.3054 - val_loss: 59.8819\n",
      "-------epoch:  99 --------\n",
      "6000/6000 [==============================] - 1s 161us/sample - loss: 59.8249\n",
      "\n",
      "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
      "4000/4000 [==============================] - 152s 38ms/step - loss: 59.2835 - val_loss: 59.8249\n"
     ]
    }
   ],
   "source": [
    "def create_model_checkpoint(dir, model_name):\n",
    "    filepath = dir + '/' + model_name + \".h5\" #-{epoch:02d}-{decoded_mean:.2f}\n",
    "    directory = os.path.dirname(filepath)\n",
    "    try:\n",
    "        os.stat(directory)\n",
    "    except:\n",
    "        os.mkdir(directory)\n",
    "    checkpointer = ModelCheckpoint(filepath=filepath, verbose=1, save_best_only=False)\n",
    "    return checkpointer\n",
    "\n",
    "checkpointer = create_model_checkpoint('models', 'vae_seq2seq')\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "\n",
    "nb_epoch=100\n",
    "n_steps = int((800000/2)/batch_size)\n",
    "for counter in range(nb_epoch):\n",
    "    print('-------epoch: ',counter,'--------')\n",
    "    history = vae.fit_generator(sent_generator(TRAIN_DATA_FILE, batch_size/2),\n",
    "                          steps_per_epoch=n_steps, epochs=1, callbacks=[checkpointer],\n",
    "                          validation_data=(data_1_val, data_1_val))\n",
    "    write_log(callback, ['loss', 'val_loss'], [history], counter)\n",
    "    \n",
    "vae.save('models/50_e1_d1_ls32_itm96.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.load_weights('models/50_e1_d1_ls32_itm96.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project and sample sentences from the latent space\n",
    "Now we build an encoder model that takes a sentence and projects it on the latent space   \n",
    "and a decoder model that goes from the latent space back to the text representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model to project sentences on the latent space\n",
    "encoder = Model(x, z_mean)\n",
    "#encoder = Model(x, z_mean)\n",
    "\n",
    "# build a generator that can sample sentences from the learned distribution\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_h_decoded = decoder_h(repeated_context(decoder_input))\n",
    "_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "_x_decoded_mean = Activation('softmax')(_x_decoded_mean)\n",
    "generator = Model(decoder_input, _x_decoded_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on validation sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word = {v: k for k, v in word_index.items()}\n",
    "sent_encoded = encoder.predict(data_1_val, batch_size = 16)\n",
    "x_test_reconstructed = generator.predict(sent_encoded)\n",
    "max_value = np.apply_along_axis(np.argmax, 1, x_test_reconstructed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  3  1 18  7  7  7  7  1  1  8  1  8 35]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 'what',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'the',\n",
       " 'the',\n",
       " 'in',\n",
       " 'the',\n",
       " 'in',\n",
       " 'india']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_idx = 672\n",
    "# Apply a function to 1-D slices along the given axis.\n",
    "reconstructed_indexes = np.apply_along_axis(np.argmax, 1, x_test_reconstructed[sent_idx])\n",
    "print(reconstructed_indexes)\n",
    "\n",
    "#np.apply_along_axis(np.max, 1, x_test_reconstructed[sent_idx])\n",
    "#np.max(np.apply_along_axis(np.max, 1, x_test_reconstructed[sent_idx]))\n",
    "word_list = list(np.vectorize(index2word.get)(reconstructed_indexes))\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 'where',\n",
       " 'can',\n",
       " 'i',\n",
       " 'find',\n",
       " 'the',\n",
       " 'full',\n",
       " 'list',\n",
       " 'of',\n",
       " 'skills',\n",
       " 'for',\n",
       " 'the',\n",
       " 'linkedin',\n",
       " 'skills',\n",
       " 'feature']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_sent = list(np.vectorize(index2word.get)(data_1_val[sent_idx]))\n",
    "original_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence processing and interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to parse a sentence\n",
    "def sent_parse(sentence, mat_shape):\n",
    "    sequence = tokenizer.texts_to_sequences(sentence)\n",
    "    padded_sent = pad_sequences(sequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    return padded_sent#[padded_sent, sent_one_hot]\n",
    "\n",
    "# input: encoded sentence vector\n",
    "# output: encoded sentence vector in dataset with highest cosine similarity\n",
    "def find_similar_encoding(sent_vect):\n",
    "    all_cosine = []\n",
    "    for sent in sent_encoded:\n",
    "        result = 1 - spatial.distance.cosine(sent_vect, sent)\n",
    "        all_cosine.append(result)\n",
    "    data_array = np.array(all_cosine)\n",
    "    maximum = data_array.argsort()[-3:][::-1][1]\n",
    "    new_vec = sent_encoded[maximum]\n",
    "    return new_vec\n",
    "\n",
    "# input: two points, integer n\n",
    "# output: n equidistant points on the line between the input points (inclusive)\n",
    "def shortest_homology(point_one, point_two, num):\n",
    "    dist_vec = point_two - point_one\n",
    "    sample = np.linspace(0, 1, num, endpoint = True)\n",
    "    hom_sample = []\n",
    "    for s in sample:\n",
    "        hom_sample.append(point_one + s * dist_vec)\n",
    "    return hom_sample\n",
    "\n",
    "# input: original dimension sentence vector\n",
    "# output: sentence text\n",
    "def print_latent_sentence(sent_vect):\n",
    "    sent_vect = np.reshape(sent_vect,[1,latent_dim])\n",
    "    sent_reconstructed = generator.predict(sent_vect)\n",
    "    sent_reconstructed = np.reshape(sent_reconstructed,[max_len,NB_WORDS])\n",
    "    reconstructed_indexes = np.apply_along_axis(np.argmax, 1, sent_reconstructed)\n",
    "    np.apply_along_axis(np.max, 1, x_test_reconstructed[sent_idx])\n",
    "    np.max(np.apply_along_axis(np.max, 1, x_test_reconstructed[sent_idx]))\n",
    "    word_list = list(np.vectorize(index2word.get)(reconstructed_indexes))\n",
    "    w_list = [w for w in word_list if w]\n",
    "    print(' '.join(w_list))\n",
    "    #print(word_list)\n",
    "        \n",
    "def new_sents_interp(sent1, sent2, n):\n",
    "    tok_sent1 = sent_parse(sent1, [15])\n",
    "    tok_sent2 = sent_parse(sent2, [15])\n",
    "    enc_sent1 = encoder.predict(tok_sent1, batch_size = 16)\n",
    "    enc_sent2 = encoder.predict(tok_sent2, batch_size = 16)\n",
    "    test_hom = shortest_homology(enc_sent1, enc_sent2, n)\n",
    "    for point in test_hom:\n",
    "        print_latent_sentence(point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the best to to to to the the in the in india\n",
      "what is the best to to to to the the in the in india\n"
     ]
    }
   ],
   "source": [
    "example_sentence = ['where can i find the full list of skills for the linkedin skills feature']\n",
    "mysent_sample = sent_parse(example_sentence, [15])\n",
    "mysent_sample_encoded = encoder.predict(mysent_sample, batch_size=16)\n",
    "print_latent_sentence(mysent_sample_encoded)\n",
    "print_latent_sentence(find_similar_encoding(mysent_sample_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "Now we can try to parse two sentences and interpolate between them generating new sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 15) (1, 32)\n",
      "how do i get a to in in quora\n",
      "how do i get a to in in quora\n",
      "how do i get my in quora\n",
      "how do i get my in quora\n",
      "-----------------\n",
      "how do i get a to in in quora\n",
      "what is the best to to in india\n",
      "how do i get a in in quora\n",
      "what is the best of in india\n",
      "how do i get a in quora\n",
      "how do i get my in quora\n"
     ]
    }
   ],
   "source": [
    "sentence1=['where can i find a book on machine learning']\n",
    "mysent = sent_parse(sentence1, [15])\n",
    "mysent_encoded = encoder.predict(mysent, batch_size = 16)\n",
    "print(mysent.shape, mysent_encoded.shape)\n",
    "print_latent_sentence(mysent_encoded)\n",
    "print_latent_sentence(find_similar_encoding(mysent_encoded))\n",
    "\n",
    "sentence2=['how can i become a successful entrepreneur']\n",
    "mysent2 = sent_parse(sentence2, [15])\n",
    "mysent_encoded2 = encoder.predict(mysent2, batch_size = 16)\n",
    "print_latent_sentence(mysent_encoded2)\n",
    "print_latent_sentence(find_similar_encoding(mysent_encoded2))\n",
    "print('-----------------')\n",
    "\n",
    "new_sents_interp(sentence1, sentence2, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "After training with these parameters for 100 epochs I got these results from interpolating between these two sentences:\n",
    "\n",
    "sentence1=['where can i find a book on machine learning']\n",
    "sentence2=['how can i become a successful entrepreneur']\n",
    "\n",
    "Generated sentences:\n",
    "- ------------------------------------------- -\n",
    "-  where can i find a book on machine learning\n",
    "-  where can i find a a machine book\n",
    "-  how can i write a a machine book\n",
    "-  how can i become a successful architect\n",
    "-  how can i become a successful architect\n",
    "-  how can i become a successful entrepreneur\n",
    "- ------------------------------------------- -\n",
    "\n",
    "As we can see the results are not yet completely satisfying because not all the sentences are grammatically correct and in the interpolation the same sentence has been generated multiple times but anyway the model, even in this preliminary version seems to start working.\n",
    "There are certainly many improvements that could be done like:\n",
    "-  removing all the sentences longer than 15 instead of just truncating them\n",
    "-  introduce the equivalent of word dropout used in the original paper for this decoder architecture \n",
    "-  parameter tuning (this model trains in few hours on a GTX950M with 2GB memory so it's definitely possible to try larger nets)\n",
    "-  Using word embeddings with higher dimensionality\n",
    "-  train on a more general dataset (Quora sentences are all questions)\n",
    "\n",
    "Stay tuned for future refinings of the model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
